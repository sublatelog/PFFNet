tools/train_net.py
main()
    parser.add_argument()
    
    # local_rank:default=0
    # distributed:num_gpus > 1
    train(cfg, args.local_rank, args.distributed)
        model = build_detection_model(cfg)
        
            # PFFNet/maskrcnn_benchmark/modeling/detector/generalized_rcnn.py
            GeneralizedRCNN(cfg)
                
                # PFFNet/maskrcnn_benchmark/modeling/backbone/backbone.py # R-101-FPN
                backbone, semseg = build_backbone(cfg)
                    body = resnet.ResNet(cfg)
                    
                    # ResNet_fpn
                    fpn = fpn_module.FPN(...)
                    model = nn.Sequential(OrderedDict([("body", body), ("fpn", fpn)])) # OrderedDict:順序付き辞書
                        in_channels_list=[
                            in_channels_stage2, # cfg.MODEL.RESNETS.RES2_OUT_CHANNELS:256
                            in_channels_stage2 * 2,
                            in_channels_stage2 * 4,
                            in_channels_stage2 * 8,
                        ],
                        out_channels=out_channels,
                    
                    # ResNet_semantic
                    # PFFNet/maskrcnn_benchmark/modeling/sem_branch/mcgcn.py
                    sem_branch = mcgcn.RGCN(in_channels_list, middle_channels, sem_out_channel, conv_block) 
                        RGCN(nn.Module):
                            for idx, in_channels in enumerate(in_channels_list, 1):
                                gcn_block  = RESBlock[x_conv_conv_out+res]
                                res_block  = GCNBlock[conv5_1_conv1_5+conv1_5_conv5_1]
                                res2_block = GCNBlock[conv5_1_conv1_5+conv1_5_conv5_1]
                                inner_block = conv1_1
                                layer_block = conv3_3
                            
                            # last_inner
                            last_inner = gcn_block__res2_block
                            
                            # C1 > C3
                            for idx, in_channels in enumerate(in_channels_list, 1):
                                # scaled_last_inner
                                inner_top_down = F.interpolate(last_inner, scale_factor=2, mode="nearest")
                                
                                # this_inner_lateral2
                                inner_lateral2 = gcn_blocks__res2_blocks

                                # last_inner
                                last_inner_pre = inner_lateral2 + inner_top_down
                                last_inner = res_blocks[-idx+1])(last_inner_pre)
                                
                                results.insert(0, last_inner)                            
                            
                    model_sem = nn.Sequential(OrderedDict([("body", body), ("semseg", sem_branch)]))
                    
                
                rpn = build_rpn(cfg, self.backbone.out_channels)
                
                    # PFFNet/maskrcnn_benchmark/modeling/rpn/rpn.py
                    RPNModule()
                        init()
                            # PFFNet/maskrcnn_benchmark/modeling/rpn/anchor_generator.py
                            anchor_generator = make_anchor_generator(cfg)
                                init(sizes=(128, 256, 512), aspect_ratios=(0.5, 1.0, 2.0), anchor_strides=(8, 16, 32), straddle_thresh=0,)
                                    cell_anchors = [
                                                    # (1,1)のanchorを作成
                                                    generate_anchors(anchor_stride, size if isinstance(size, (tuple, list)) else (size,), aspect_ratios).float()
                                                    for anchor_stride, size in zip(anchor_strides, sizes)
                                                    ]
                                                
                                forward(image_list, feature_maps)                                
                                    # grid_sizes
                                    grid_sizes = [feature_map.shape[-2:] for feature_map in feature_maps]
                                    
                                    # all anchor
                                    anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)
                                        shifts_x = torch.arange(0, grid_width * stride, step=stride, dtype=torch.float32, device=device)
                                        shifts_y = torch.arange(0, grid_height * stride, step=stride, dtype=torch.float32, device=device)
                                        shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
                                        shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)
                                        anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))

                                    # image > feature_map > boxlist
                                    for i, (image > _height, image_width) in enumerate(image_list.image_sizes):
                                        for anchors_per_feature_map in anchors_over_all_feature_maps:
                                            # 
                                            boxlist = BoxList(anchors_per_feature_map, (image_width, image_height), mode="xyxy")
                                            self.add_visibility_to(boxlist)
                                                inds_inside = 0 or 1
                                            anchors_in_image.append(boxlist)
                                        anchors.append(anchors_in_image)

                            # PFFNet/maskrcnn_benchmark/config/defaults.py
                            # rpn_head:cfg.MODEL.RPN.RPN_HEAD:"SingleConvRPNHead"
                            head = rpn_head(cfg, in_channels, anchor_generator.num_anchors_per_location()[0])                           
                                

                            # PFFNet/maskrcnn_benchmark/modeling/box_coder.py
                            rpn_box_coder = BoxCoder(weights=(1.0, 1.0, 1.0, 1.0))                        

                            # PFFNet/maskrcnn_benchmark/modeling/rpn/inference.py
                            box_selector_train = make_rpn_postprocessor(cfg, rpn_box_coder, is_train=True)
                                fpn_post_nms_top_n = 2000
                                pre_nms_top_n = 1000
                                post_nms_top_n = 500
                                fpn_post_nms_per_batch = True
                                nms_thresh = 0.7
                                min_size = 0
                                box_selector = RPNPostProcessor(pre_nms_top_n, post_nms_top_n, nms_thresh, min_size, rpn_box_coder, fpn_post_nms_top_n, fpn_post_nms_per_batch,)
                                    for a, o, b in zip(anchors, objectness, box_regression):
                                        sampled_boxes.append(self.forward_for_single_feature_map(a, o, b))
                                            forward_for_single_feature_map(a, o, b)
                                                N, A, H, W = objectness.shape
                                                objectness = permute_and_flatten(objectness, N, A, 1, H, W).view(N, -1)
                                                objectness = objectness.sigmoid()
                                                
                                                box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
                                                
                                                objectness, topk_idx = objectness.topk(pre_nms_top_n, dim=1, sorted=True)
                                                
                                                proposals = self.box_coder.decode(box_regression.view(-1, 4), concat_anchors.view(-1, 4))
                                                
                                                for proposal, score, im_shape in zip(proposals, objectness, image_shapes):
                                                    boxlist = BoxList(proposal, im_shape, mode="xyxy")
                                                    boxlist.add_field("objectness", score)
                                                    
                                                    # clip_to_image
                                                    boxlist = boxlist.clip_to_image(remove_empty=False)
                                                    
                                                    # remove_small
                                                    boxlist = remove_small_boxes(boxlist, self.min_size)
                                                    
                                                    # nms
                                                    boxlist = boxlist_nms(
                                                        boxlist,
                                                        self.nms_thresh,
                                                        max_proposals=self.post_nms_top_n,
                                                        score_field="objectness",
                                                    )
                                                    result.append(boxlist)
                                    
                                    boxlists = [cat_boxlist(boxlist) for boxlist in boxlists]
                                    
                                    boxlists = select_over_all_levels(boxlists)
                                    
                                    boxlists = add_gt_proposals(boxlists, targets)


                            loss_evaluator = make_rpn_loss_evaluator(cfg, rpn_box_coder)
                        
                            
                        forward(self, images, features, targets=None):
                            objectness, rpn_box_regression, feat_rpn_scales = rpn_head(features)
                                for feature in x:
                                    t = F.relu(conv(feature))
                                    feat_scales.append(t) # nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)
                                    logits.append(cls_logits_cov(t)) # (in_channels, num_anchors, kernel_size=1, stride=1)
                                    bbox_reg.append(self.bbox_pred_cov(t)) # nn.Conv2d(in_channels, num_anchors * 4, kernel_size=1, stride=1)
                                    
                                return logits, bbox_reg, feat_scales

                            _forward_train(self, feat_rpn_scales, anchors, objectness, rpn_box_regression, targets):

                                boxes = self.box_selector_train(anchors, objectness, rpn_box_regression, targets)

                                loss_objectness, loss_rpn_box_reg = self.loss_evaluator(anchors, objectness, rpn_box_regression, targets)
                
                
                
                roi_heads = build_roi_heads(cfg, self.backbone.out_channels)
                
                    # PFFNet/maskrcnn_benchmark/modeling/roi_heads/box_head/box_head.py 
                    roi_heads.append(("box", build_roi_box_head(cfg, in_channels)))
                        proposals = self.loss_evaluator.subsample(proposals, targets)
                        
                        x_pred = self.feature_extractor(features, proposals)
                        
                        class_logits, box_regression = self.predictor(x_pred)
                        
                        loss_classifier, loss_box_reg = self.loss_evaluator([class_logits], [box_regression])
                        
                        bbox_tr = self.ff_box_processor((class_logits, box_regression), proposals)

                        bbox_tr = bbox_tr[0].convert("xyxy")
                        
                    
                    # PFFNet/maskrcnn_benchmark/modeling/roi_heads/mask_head/mask_head.py
                    roi_heads.append(("mask", build_roi_mask_head(cfg, in_channels)))
                    
                        proposals, positive_inds, positive_bboxes = keep_only_positive_boxes(proposals, bboxes_train)
                        
                        feature_fcn_conv4 = x[torch.cat(positive_inds, dim=0)]
                        
                        mask_logits = self.predictor(feature_fcn_conv4, mask_fc_feature)
                        
                        proposal_boxes = positive_bboxes
                        
                        roi_boxes = proposal_boxes[0]

                        loss_mask, selected_mask, labels, maskiou_targets = self.loss_evaluator(proposals, mask_logits, targets)
                        
                        return 
                            feature_fcn_conv4, 
                            all_proposals, 
                            dict(loss_mask=loss_mask), 
                            mask_logits, 
                            roi_boxes, 
                            roi_feature, 
                            selected_mask, 
                            labels, 
                            maskiou_targets
                    
                    
                    # PFFNet/maskrcnn_benchmark/modeling/roi_heads/maskiou_head/roi_maskiou_predictors.py
                    roi_heads.append(("maskiou", build_roi_maskiou_head(cfg)))
                        nn.Linear(1024, num_classes)
                        
                    # PFFNet/maskrcnn_benchmark/modeling/roi_heads/keypoint_head/keypoint_head.py
                    roi_heads.append(("keypoint", build_roi_keypoint_head(cfg, in_channels)))
                        proposals = self.loss_evaluator.subsample(proposals, targets)
                        
                        x = self.feature_extractor(features, proposals)
                        kp_logits = self.predictor(x)
                        
                        loss_kp = self.loss_evaluator(proposals, kp_logits)

                        return x, proposals, dict(loss_kp=loss_kp)
                        
                        
                    # PFFNet/maskrcnn_benchmark/modeling/roi_heads/roi_heads.py
                    roi_heads = CombinedROIHeads(cfg, roi_heads)
                        x, detections, loss_mask, masks, bboxes, roi_feature, selected_mask, labels, maskiou_targets 
                        = self.mask(mask_features, detections, bboxes_train, targets)
                        losses.update(loss_mask)
                
                
                
                roi_feature_fusion = build_roi_feature_fusion(cfg)
                
                
                
        
        optimizer = make_optimizer(cfg, model)
        scheduler = make_lr_scheduler(cfg, optimizer)
        
        # FP16の演算を前提とした(順および逆)伝播計算等を支援する関数
        model, optimizer = amp.initialize(model, optimizer, opt_level=amp_opt_level)
        
        checkpointer = DetectronCheckpointer(cfg, model, optimizer, scheduler, output_dir, save_to_disk)
        
        data_loader = make_data_loader(cfg, is_train=True, is_distributed=distributed, start_iter=arguments["iteration"],)
        data_loader_val = make_data_loader(cfg, is_train=False, is_distributed=distributed, is_for_period=True)
        
        do_train(cfg, model, data_loader, data_loader_val, optimizer, scheduler, checkpointer, device, checkpoint_period, test_period, arguments, )
            # GeneralizedRCNN
            model.forward(self, images, targets=None, semgt = None)
                features_all = self.backbone(images.tensors)
                
                # feature
                features = features_all[1:]
                
                # semantic
                fpn_semantic_pred = features_all[0]
                pred_sembranch = self.semseg(images.tensors)
                
                # rpn
                proposals, proposal_losses, feat_rpn_scales = self.rpn(images, features, targets)
                
                # roi
                x, result, detector_losses, masks, bboxes = self.roi_heads(features, proposals, targets)
                
                # loss detector, proposal
                losses = {}
                losses.update(detector_losses)
                losses.update(proposal_losses)
                
                # loss semantic
                sembranch_loss = nn.CrossEntropyLoss()(pred_sembranch, gt_semseg)
                sembranch_losses = {'loss_sem_branch': sembranch_loss}
                losses.update(sembranch_losses)
                
                # loss mask
                # PFFNet/maskrcnn_benchmark/modeling/roi_heads/feature_fusion.py
                pred_maskfusion = self.roi_feature_fusion(masks, bboxes, fpn_semantic_pred)
                    for mask, box in zip(masks.unsqueeze(1), boxes.bbox):
                    
                        # resize
                        box = box.numpy().astype(np.int32)
                        _, _, im_h, im_w = feature_map.size()
                        TO_REMOVE = 1
                        w = box[2] - box[0] + TO_REMOVE
                        h = box[3] - box[1] + TO_REMOVE
                        w = max(w, 1)
                        h = max(h, 1)                        
                        resized_mask = interpolate(mask, [w, h], mode='bilinear')
                        
                        # crop
                        x_0 = box[0]
                        x_1 = min(box[2] + 1, im_w)
                        y_0 = box[1]
                        y_1 = min(box[3] + 1, im_h)
                        mask_cropped = resized_mask[:, :,0: min(w, im_w - y_0), 0: min(h, im_h - x_0)]
                    
                        # attention
                        mask_cropped_att = mask_cropped.sigmoid().detach()
                        feature_map[:, :, y_0: min(y_0 + w, im_w), x_0: min(x_0 + h, im_h)] *= (1 + mask_cropped_att)
                        
                    out_sem_feature_fusion = self.cov_gn_rel_3x3(F.relu(feature_map))
                    out_sem_feature_fusion = self.cov_gn_rel_1x1(F.relu(out_sem_feature_fusion))
                    
                    
                    
                maskfusion_loss = nn.CrossEntropyLoss()(pred_maskfusion, gt_semseg)
                maskfusion_losses = {'loss_mask_fusion': maskfusion_loss}
                losses.update(maskfusion_losses)
                
                # loss semantic_mask
                prob_sembranch = nn.Softmax(dim=1)(pred_sembranch)
                prob_maskfusion = nn.Softmax(dim=1)(pred_maskfusion)
                sem_consistence_loss = nn.MSELoss()(prob_sembranch, prob_maskfusion) * self.sem_consistence_lambda
                sem_consistence_losses = {'loss_sem_consistence': sem_consistence_loss}
                losses.update(sem_consistence_losses)
                
                return losses
                













------------------------------------------------------------------------------------------------------------------------------------------------
configs/biomed_seg/e2e_mask_rcnn_R_101_FPN_1x_gn-tnbc.yaml

MODEL:
  META_ARCHITECTURE: "GeneralizedRCNN"
  WEIGHT: "catalog://ImageNetPretrained/MSRA/R-101-GN"
  BACKBONE:
    CONV_BODY: "R-101-FPN"
  RESNETS: # use GN for backbone
    BACKBONE_OUT_CHANNELS: 256
    STRIDE_IN_1X1: False
    TRANS_FUNC: "BottleneckWithGN"
    STEM_FUNC: "StemWithGN"
  FPN:
    USE_GN: True # use GN for FPN
  RPN:
    USE_FPN: True
    ANCHOR_STRIDE: (4, 8, 16, 32, 64)
    ANCHOR_SIZES: (32, 64, 128, 256, 512)
    PRE_NMS_TOP_N_TRAIN: 1000
    POST_NMS_TOP_N_TRAIN: 500
    PRE_NMS_TOP_N_TEST: 4000
    POST_NMS_TOP_N_TEST: 2000
    FPN_POST_NMS_TOP_N_TEST: 1000
  ROI_HEADS:
    USE_FPN: True
    BATCH_SIZE_PER_IMAGE: 256
    POSITIVE_FRACTION: 0.25
    NMS: 0.3
    DETECTIONS_PER_IMG: 500
  ROI_BOX_HEAD:
    USE_GN: True # use GN for bbox head
    POOLER_RESOLUTION: 7
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    POOLER_SAMPLING_RATIO: 2
    FEATURE_EXTRACTOR: "FPN2MLPFeatureExtractor"
    PREDICTOR: "FPNPredictor"
    NUM_CLASSES: 2
  ROI_MASK_HEAD:
    USE_GN: True # use GN for mask head
    POOLER_SCALES: (0.25, 0.125, 0.0625, 0.03125)
    CONV_LAYERS: (256, 256, 256, 256)
    FEATURE_EXTRACTOR: "MaskRCNNFPNFeatureExtractor"
    PREDICTOR: "MaskRCNNC4Predictor"
    POOLER_RESOLUTION: 14
    POOLER_SAMPLING_RATIO: 2
    RESOLUTION: 28
    SHARE_BOX_FEATURE_EXTRACTOR: False
    DUAL_MODAL: True
  MASK_ON: True
  MASKIOU_ON: True
  PANOPTIC:
    PS_ON: True
    SEM_PATH: "./datasets/cell-tnbc/semgt"
    ATTENTION_FUSION: True
    SEMANTIC_CONSISTENCE: True
    SEMANTIC_CONSISTENCE_LAMBDA: 1.0
DATASETS:
  TRAIN: ("cell_tnbc",)
  TEST: ("cell_tnbc",)
DATALOADER:
  SIZE_DIVISIBILITY: 32
SOLVER:
  # Assume 8 gpus
  BASE_LR: 0.003
  WEIGHT_DECAY: 0.0001
  STEPS: (9000, 12000)
  MAX_ITER: 12000
  IMS_PER_BATCH: 1
  EPOCH_PERIOD: 1500
  CHECKPOINT_PERIOD: 100000
TEST:
  IMS_PER_BATCH: 1
INPUT:
  MAX_SIZE_TRAIN: 256
  MAX_SIZE_TEST: 512
  PIXEL_MEAN: [128.0, 128.0, 128.0] # coco default: [102.9801, 115.9465, 122.7717]
  HORIZONTAL_FLIP_PROB_TRAIN: 0.5
  VERTICAL_FLIP_PROB_TRAIN: 0.5
